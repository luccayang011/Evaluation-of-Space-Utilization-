<h1>Evaluating the Space Utilization of Warehouses</h1>

<b>All sensitive information is removed to ensure data security.</b>

<p>These are the demo codes I've cut and taken from a project I've done during one of my internships. The goal is to evaluate the space utilization efficiency of warehouses. To achieve this, I've discussed with various stakeholders, and then obtained and defined a set of key metrics. </p>

<p>In <a href = "https://github.com/luccayang011/Project-Evaluating_Warehouse_Space_Utilization/blob/main/Pandas_Demo_Analysis.ipynb">Pandas_Demo_Analysis.ipynb</a> I first used the regular data analysis packages Numpy & Pandas to understand and analyze sample data. And then I used them to build and validate the calculation method for those metrics.</p>

<p> <a href = "https://github.com/luccayang011/Project-Evaluating_Warehouse_Space_Utilization/blob/main/PySpark_Ingest_Transform_demo.ipynb">PySpark_Ingest_Transform_demo.ipynb</a> shows how the data being ingested from data lake via Databricks, transformed, and then outputted as tables so PowerBI can read and visualize.</p>

<p> <a href = https://github.com/luccayang011/Project-Evaluating_Warehouse_Space_Utilization/blob/main/Product_Class_Analysis.ipynb">Product_Class_Analysis.ipynb</a> is just a small individual project, in which I was trying to use the current data to predict product movement class.</p>

<h2>Reference</h2>
A very useful course I've followed to learn how to use Databricks:<a href = "https://www.udemy.com/course/azure-databricks-spark-core-for-data-engineers/
">Azure Databricks & Spark For Data Engineers (PySpark / SQL)</a>

<br>
<br>
<hr>

<h3>Something Interesting</h3>
<p><b>Here are my original plain words:</b></p>
<p>All the sensitive information is removed from demo codes to ensure data security.</p>
<p>This comes from one of my projects. The goal is to define several key metrics to evaluate the current space utilization efficiency for warehouses. </p>
<p>I used numpy & pandas to analyze the sample data first. Once testify the success of all the calculation methods, I used PySpark in Databricks to read the data from Data Lake, performed data transformation, and output results as tables. </p>


<p><b>After input to ChatGPT, here are what it gave:</b></p>
<p>
In one of my recent projects, the objective was to evaluate the space utilization efficiency of warehouses by defining key metrics. To accomplish this, I utilized numpy and pandas libraries to conduct an initial analysis on a sample dataset. Once I confirmed the accuracy of the calculation methods, I leveraged PySpark within Databricks to read data from the data lake, perform data transformations, and generate result tables.
</p>
<p>
By utilizing numpy and pandas, I was able to efficiently analyze the sample data, ensuring the validity of the calculation methods. The libraries provided a wide range of functionalities for data manipulation and statistical analysis, enabling me to extract valuable insights from the dataset.
</p>
<p>
To scale up the analysis and process larger volumes of data, I employed PySpark in the Databricks environment. PySpark's distributed computing capabilities allowed me to efficiently read data from the data lake and leverage the power of Spark for performing data transformations. This approach ensured that the analysis could be conducted on a larger scale while maintaining performance and scalability.
</p>

Finally, I produced the desired results in the form of tables, which provided a clear overview of the key metrics related to warehouse space utilization efficiency. The tables served as valuable resources for stakeholders to assess and make informed decisions based on the insights obtained.

Throughout the project, my focus was on maintaining data security by removing any sensitive information from the code and adhering to best practices for handling and protecting data.
